---
title: "Working with Plant Water Potential Data in DuckDB"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Working with Plant Water Potential Data in DuckDB}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE  # Set to FALSE since we don't have actual data during package build
)
```

```{r setup}
library(PSInetR)
library(dplyr)
library(DBI)
library(duckdb)
library(lubridate)
```

## Introduction

This vignette demonstrates how to work with plant water potential data stored in DuckDB format. DuckDB is a high-performance analytical database system that works well with R and provides efficient querying capabilities for plant water potential (Î¨) datasets.

For more information about DuckDB and its capabilities, see the [official DuckDB documentation](https://duckdb.org/docs/).

### Why DuckDB?

DuckDB offers several advantages for working with plant water potential data:

1. **Performance**: DuckDB provides fast analytical queries, even on larger datasets
2. **Ease of use**: No separate server installation required
3. **SQL and dplyr compatibility**: Familiar query syntax options
4. **Zero configuration**: Works immediately with R (or Python)
5. **Column-oriented storage**: Optimized for analytical workloads common in ecological research

## Connecting to the Database

First, let's download the DuckDB data (if you haven't already) and connect to it:

```{r connect}
# Download the data (if not already available)
data_path <- get_psi_data(format = "duckdb")

# If you already have it, get the path to the db
path_to_db <- get_db_path()

# Connect to the DuckDB database
con <- dbConnect(duckdb::duckdb(), path_to_db)

# List available tables
tables <- dbListTables(con)
print(tables)
```

## Understanding the Database Structure

For comprehensive details about all variables, data types, and units, see the [PSInetDB data schema documentation](https://github.com/PSInetRCN/PSInetDB/blob/main/docs/schema/data_schema.md).

### Database Creation and Scope

The PSInet database was created from individual data submissions encompassing plant water potential time series, where water potential measurements were collected either manually using pressure chambers or automatically with sensors. For inclusion in the database, we defined a time series as more than one measurement on the same experimental unit, whether that was an individual plant or group of plants. 

The database comprises both observational and manipulative experiments, conducted either in situ or in controlled environments such as glasshouses and growth chambers. We encouraged the submission of associated soil and meteorological time series that temporally overlap with the reported plant water potentials, though this was not required for inclusion.

### Dataset Naming Conventions

Datasets in PSInet follow specific naming conventions based on their source:

- **Direct submissions to PSInet**: Named using the first three letters of the submitter's last name, followed by a number (e.g., "And_1", "Smi_2")
- **SAPFLUXNET collections**: Retain the original site name used by SAPFLUXNET for consistency with that network

Most studies measured water potential repeatedly at a single location, but some conducted measurements at multiple sites as part of a single study. For multi-site studies, the naming convention includes a letter after the number (e.g., "Pel_2a") and the variable `multi_site` in the `study_site` table is set to TRUE.

### Database Organization

The PSInet database is organized around **6 core tables** containing information about data collection and experimental design, and **5 measurement tables** containing the actual measurements. Additionally, one core and each measurement table has an associated **quality control flag table**, and there is a table of SAPFLUXNET IDs for cross-compatibility. 

#### Core Tables

These tables contain metadata and experimental design information. **Important note**: Authors contributed their own `plot_id` and `individual_id` values, so these variables are not named consistently across the database. To correctly interpret plant water potential values, they must be joined with the appropriate subset of these core tables.

#### Measurement Tables

The database includes five types of measurement tables:
- **chamber_wp**: Manual pressure chamber water potential measurements
- **auto_wp_sensor**: Sensor details associated with automated water potential
- **auto_wp**: Automated water potential measurements  
- **soil_var**: Soil water content and soil water potential measurements
- **met_var**: Meteorological variables


Due to the diverse array of experimental designs, measurements can be reported at different organizational scales:

- **Pressure chamber water potentials**: Are associated with individuals or groups of individuals as listed in the `plant` table. Can be reported as either single measurements, or the mean of multiple measurements 
- **Soil variables**: 
Can be associated with either individual plants or plots (including the whole study)
- **Meteorological variables**: Representative of the entire study site
- **Automated water potentials**: Are reported for individual plants, but individual plants may be instrumented with multiple sensors

Therefore, `auto_wp` measurements must be interpreted in conjunction with `auto_wp_sensor`, which reports detailed information about each sensor installation, including duration, location, and calibration details.

#### Quality Control Flag Tables

Each core table (`plant`) and all five measurement tables have an associated flag table (e.g., `chamber_wp_flag`, `auto_wp_flag`, etc.). Flag tables are identical in dimensions to their corresponding data tables and contain only logical TRUE/FALSE values. These values indicate whether each entry passes our quality control checks, including range checks that can be found in the [PSInetDB data schema documentation](https://github.com/PSInetRCN/PSInetDB/blob/main/docs/schema/data_schema.md).

**Important note**: We used the same range checks as SAPFLUXNET to maintain consistency across networks. However, values flagged as FALSE should not be automatically discarded. Some FALSE flags indicate instrument drift, while others represent measurements from sites with known extreme conditions (e.g., exceptionally high vapor pressure deficit or low relative humidity). All values included in the database have been confirmed with the original data submitter. It is up to the database user's discretion whether to retain or omit flagged data. 

#### SAPFLUXNET Integration

Data originating from SAPFLUXNET have an additional `sapfluxnet` table, which records the plant name and code as originally used in the SAPFLUXNET database, facilitating cross-network data integration and comparison.

### Exploring the Database Structure

Let's examine the database structure programmatically:

```{r explore_tables}
# List all tables in the database
dbListTables(con)

# Look at the database schema
db_schema <- DBI::dbGetQuery(con, "
  SELECT 
    table_name, 
    column_name, 
    data_type
  FROM information_schema.columns 
  WHERE table_schema = 'main'
  ORDER BY table_name, ordinal_position
")

# View the schema of a specific table
chamber_wp_fields <- dbListFields(con, "chamber_wp")
print(chamber_wp_fields)

# Check the dimensions of core tables
core_tables <- c("study_site", "plot", "plant", "treatment", "data_description", "addtl_data")
for(table in core_tables) {
  count <- dbGetQuery(con, paste("SELECT COUNT(*) as count FROM", table))
  cat(table, ":", count$count, "records\n")
}

# Check the dimensions of measurement tables  
measurement_tables <- c("chamber_wp", "auto_wp", "soil_var", "met_var", "auto_wp_sensor")
for(table in measurement_tables) {
  count <- dbGetQuery(con, paste("SELECT COUNT(*) as count FROM", table))
  cat(table, ":", count$count, "records\n")
}
```

## Basic Querying

Now that we understand the database structure, let's explore the data with some basic queries using dplyr syntax (we'll talk more about dplyr syntax here in a second):

```{r basic_queries}
# Get a list of all study sites
study_sites <- tbl(con, "study_site") |>
  select(dataset_name, latitude_wgs84, longitude_wgs84) |>
  head(10) |>
  collect()
print(study_sites)

# Get plant species information
plant_species <- tbl(con, "plant") |>
  select(genus, specific_epithet) |>
  distinct() |>
  head(10) |>
  collect()
print(plant_species)

# Get statistics on water potential measurements
wp_stats <- tbl(con, "chamber_wp") |>
  filter(!is.na(water_potential_mean)) |>
  summarize(
    count = n(),
    avg_potential = mean(water_potential_mean, na.rm = TRUE),
    min_potential = min(water_potential_mean, na.rm = TRUE),
    max_potential = max(water_potential_mean, na.rm = TRUE)
  ) |>
  collect()
print(wp_stats)
```

## Using dplyr with DuckDB

Above, we show dplyr syntax to work with the database (whereas our first queries were with raw SQL), which often makes complex queries more readable. This functionality is enabled by the **dbplyr** package, which acts as a bridge between dplyr's data manipulation syntax and database systems.

**How it works:**
- **dplyr** provides the familiar data wrangling functions (`filter()`, `select()`, `group_by()`, etc.)
- **dbplyr** translates these dplyr commands into SQL queries that run on the database
- **DuckDB** (and many other databases like PostgreSQL, MySQL, SQLite) can be used with this approach

This means you can use the same dplyr syntax you know from working with data.frames, but the operations actually happen in the database, which is much more efficient for large datasets. The translation to SQL happens automatically behind the scenes.

```{r dplyr_queries}
# Create references to the tables
chamber_wp_tbl <- tbl(con, "chamber_wp")
plant_tbl <- tbl(con, "plant")
plot_tbl <- tbl(con, "plot")

# Query using dplyr syntax for water potential by species
wp_by_species <- chamber_wp_tbl |>
  inner_join(plant_tbl, by = c("dataset_name", "individual_id")) |>
  group_by(genus, specific_epithet) |>
  summarize(
    count = n(),
    avg_potential = mean(water_potential_mean, na.rm = TRUE),
    min_potential = min(water_potential_mean, na.rm = TRUE),
    max_potential = max(water_potential_mean, na.rm = TRUE)
  ) |>
  arrange(desc(count)) |>
  collect()  # This executes the query and brings results into R

print(wp_by_species)

# Find measurements by organ type
organ_summary <- chamber_wp_tbl |>
  group_by(organ, canopy_position) |>
  summarize(
    count = n(),
    avg_potential = mean(water_potential_mean, na.rm = TRUE),
    avg_n_samples = mean(water_potential_n, na.rm = TRUE)
  ) |>
  ungroup() |>
  arrange(desc(count)) |>
  collect()

print(organ_summary)
```

## Advanced Analyses

Here are some examples of more advanced analyses that combine multiple data tables:

```{r advanced_analyses}
# Analyze seasonal patterns in water potential
seasonal_patterns <- chamber_wp_tbl |>
  # must collect first because substr can't be run by db
  collect() |>
  # Extract month from date (YYYYMMDD format)
  mutate(month = month(date)) |>
  group_by(month) |>
  summarize(
    avg_potential = mean(water_potential_mean, na.rm = TRUE),
    sd_potential = sd(water_potential_mean, na.rm = TRUE),
    sample_count = n()
  ) 

print(seasonal_patterns)

# Comparing water potential with soil variables
soil_wp_comparison <- chamber_wp_tbl |>
  # Join with soil data on dataset_name, plot_id, and date
  inner_join(
    tbl(con, "soil_var"),
    by = c("dataset_name", "plot_id", "date")
  ) |>
  # Group by soil moisture categories (using a window function)
  mutate(
    swc_shallow_category = case_when(
      swc_mean_shallow < 0.1 ~ "very low",
      swc_mean_shallow < 0.2 ~ "low",
      swc_mean_shallow < 0.3 ~ "medium",
      TRUE ~ "high"
    )
  ) |>
  group_by(swc_shallow_category) |>
  summarize(
    avg_water_potential = mean(water_potential_mean, na.rm = TRUE),
    sd_water_potential = sd(water_potential_mean, na.rm = TRUE),
    n_observations = n()
  ) |>
  collect()

print(soil_wp_comparison)

# Analyzing the relationship between meteorological variables and water potential
met_wp_comparison <- chamber_wp_tbl |>
  # Join with meteorological data on dataset_name and date
  inner_join(
    tbl(con, "met_var"),
    by = c("dataset_name", "date")
  ) |>
  # Select relevant columns
  select(
    dataset_name, date, water_potential_mean, water_potential_sd,
    vapor_pressure_deficit_k_pa, air_temperature_c, precipitation_mm
  ) |>
  # collect before running summarize to make sure cor works correctly
  collect() |>
  # Calculate correlations
  summarize(
    vpd_correlation = cor(water_potential_mean, vapor_pressure_deficit_k_pa, 
                           use = "complete.obs"),
    temp_correlation = cor(water_potential_mean, air_temperature_c, 
                            use = "complete.obs"),
    precip_correlation = cor(water_potential_mean, precipitation_mm, 
                              use = "complete.obs"),
    n_observations = n()
  ) 

print(met_wp_comparison)

```

## Visualization Examples

Let's create some visualizations with the data we've retrieved:

```{r visualizations, message=FALSE, warning=FALSE, fig.height=8, fig.width=10}
library(ggplot2)
library(lubridate)

# Get water potential data for visualization
wp_time_data <- tbl(con, "chamber_wp") |>
  filter(!is.na(water_potential_mean)) |>
  collect() |>
  mutate(date_parsed = ymd(as.character(date)))  # Convert YYYYMMDD to Date

# Get water potential by species
wp_species <- tbl(con, "chamber_wp") |>
  inner_join(tbl(con, "plant"), by = c("dataset_name", "individual_id", "plot_id")) |>
  group_by(genus, specific_epithet) |>
  summarize(
    mean_wp = mean(water_potential_mean, na.rm = TRUE),
    sd_wp = sd(water_potential_mean, na.rm = TRUE),
    count = n()
  ) |>
  filter(count > 5) |>  # Only species with enough measurements
  collect()

# Create species name
wp_species$species <- paste(wp_species$genus, wp_species$specific_epithet)

# Plot water potential by species
ggplot(wp_species, aes(x = reorder(species, mean_wp), y = mean_wp)) +
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymin = mean_wp - sd_wp, ymax = mean_wp + sd_wp), width = 0.2) +
  labs(
    title = "Average Water Potential by Species",
    x = "Species",
    y = "Water Potential (MPa)"
  ) +
  coord_flip() +  # Flip coordinates for better readability
  theme_minimal()

# Don't forget to disconnect
dbDisconnect(con, shutdown = TRUE)
```

## Working with Both Automated and Manual Measurements

The PSInet database contains both automated continuous measurements and manual pressure chamber measurements. Here's how to work with both:

```{r combined_measurements}
# Connect to database
con <- dbConnect(duckdb::duckdb(), get_db_path())

# Combine automated and manual measurements using dplyr
# First get automated measurements
auto_data <- tbl(con, "auto_wp") |>
  inner_join(tbl(con, "plant"), by = c("dataset_name", "individual_id")) |>
  filter(!is.na(water_potential_mean)) |>
  select(dataset_name, individual_id, date, time, 
         water_potential_mean, water_potential_sd, genus, specific_epithet) |>
  mutate(measurement_type = "automated") |>
  head(500) |>  # Limit to avoid memory issues
  collect()

# Then get manual measurements  
manual_data <- tbl(con, "chamber_wp") |>
  inner_join(tbl(con, "plant"), by = c("dataset_name", "individual_id")) |>
  filter(!is.na(water_potential_mean)) |>
  select(dataset_name, individual_id, date, time,
         water_potential_mean, water_potential_sd, genus, specific_epithet) |>
  mutate(measurement_type = "manual") |>
  head(500) |>  # Limit to avoid memory issues
  collect()

# Combine the datasets
combined_data <- bind_rows(auto_data, manual_data)

# Compare automated vs manual measurements
measurement_summary <- combined_data |>
  group_by(measurement_type) |>
  summarize(
    count = n(),
    avg_potential = mean(water_potential_mean, na.rm = TRUE),
    sd_potential = sd(water_potential_mean, na.rm = TRUE),
    min_potential = min(water_potential_mean, na.rm = TRUE),
    max_potential = max(water_potential_mean, na.rm = TRUE)
  )

print(measurement_summary)

# Disconnect
dbDisconnect(con, shutdown = TRUE)
```

## Performance Tips

When working with larger PSInet datasets:

1. **Limit data retrieval**: Use `filter()` before `collect()` to reduce data transferred
2. **Use lazy evaluation**: Keep operations in the database as long as possible
3. **Index columns**: If you modify the database, consider adding indices for frequently filtered columns
4. **Close connections**: Always disconnect when done to free resources
5. **Avoid large JOINs in R**: Perform joins in DuckDB with `inner_join()` before collecting

```{r performance_example}
# Example of good practice
con <- dbConnect(duckdb::duckdb(), get_db_path())

# Efficient query - filtering happens in database
efficient_query <- tbl(con, "chamber_wp") |>
  filter(water_potential_mean < -1.0) |>
  select(dataset_name, individual_id, date, water_potential_mean) |>
  collect()

# Disconnect
dbDisconnect(con, shutdown = TRUE)
```

## Conclusion

This vignette demonstrated how to work with the PSInet database in DuckDB format. You've learned how to:

1. Connect to the DuckDB database
2. Explore the database structure
3. Perform basic and advanced queries using SQL and dplyr
4. Join related tables to analyze relationships
5. Visualize water potential data
6. Compare automated and manual measurements

DuckDB provides efficient querying capabilities that work well with both SQL and dplyr syntax, making it a powerful tool for analyzing plant water potential data. As the PSInet database grows, these techniques will help you leverage this valuable resource for ecological research and drought studies.
